{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhpN9fdpYxnK"
      },
      "source": [
        "# 実践その参（物体認識パート）\n",
        "Yolo_v3を使って物体認識モデルをトレーニングする\n",
        "\n",
        "\n",
        "\n",
        "## 条件\n",
        "- https://public.roboflow.com/ のパブリックデータセットから気になるデータセット（アライグマ以外）をダウンロード、あるいは https://www.makesense.ai/ のようなアノテーションツールで独自のデータセットを作成、それを使って学習を行うこと。\n",
        "- 学習のループは自分で書くこと\n",
        "\n",
        "\n",
        "コードは基本的に下記レポジトリから拝借しています。  \n",
        "https://github.com/DeNA/PyTorch_YOLOv3\n",
        "\n",
        "もし上記が無くなっていたら   \n",
        "https://github.com/kouohhashi/PyTorch_YOLOv3\n",
        "\n",
        "\n",
        "\n",
        "データは以下のウェブサイトから拝借しています。  \n",
        "https://public.roboflow.com/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umPhzok8YxnM"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMyxDC6NYxnN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOracsW1XldX"
      },
      "outputs": [],
      "source": [
        "# ここにStudent IDを記載しておいてください。\n",
        "Student_ID = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zn0ivf1OXldY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c-7KH14YxnO"
      },
      "source": [
        "## 1. データの準備\n",
        "\n",
        "自分のデータセットを準備する。  \n",
        "\n",
        "方法は問わないが、150件以上用意すること。\n",
        "\n",
        "https://public.roboflow.com/ のアライグマのデータセットはサンプルとして使われているのでNG。  \n",
        "全体としてスクリプトがどう動作するのか確認するのに使ってください。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3t1cgurJYxnO"
      },
      "outputs": [],
      "source": [
        "# 以下はアライグマのサンプル\n",
        "\n",
        "# アライグマのデータセットをダウンロードする\n",
        "# ! wget https://payloadcms.shabelab.com/assets/datasets/racoon.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlCg0B0wZLRA"
      },
      "outputs": [],
      "source": [
        "# 以下はアライグマのサンプル\n",
        "\n",
        "# ダウンロードしたデータを解凍する\n",
        "# ! tar xvzf racoon.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lk67rNF_YxnP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCUVSB_NaD6_"
      },
      "source": [
        "# データセットの準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_DvemxpaCEJ"
      },
      "outputs": [],
      "source": [
        "# 以下はアライグマのサンプル\n",
        "# 自分のデータでみてみましょう\n",
        "\n",
        "# サンプルを一つみてみましょう\n",
        "\n",
        "from skimage import io\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "\n",
        "im = Image.open(\"./racoon/images/raccoon-9_jpg.rf.pzESFoTi2TZ2OzvX43ep.jpg\")\n",
        "\n",
        "# Create figure and axes\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.imshow(im)\n",
        "\n",
        "rect = patches.Rectangle((10, 7), 336, 464, linewidth=6, edgecolor='r', facecolor='none')\n",
        "\n",
        "# Display the image\n",
        "ax.add_patch(rect)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NolYY0mIaZZ9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHQctTDGXldd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQpvqD5UXldd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vKIi7HgXldd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56XOX2KpXldd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kpb81s_-Xlde"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuwCzUJgXlde"
      },
      "source": [
        "# データセットの準備\n",
        "\n",
        "以下にアライグマのサンプル（意図的に他のYoloのデータセットとか少し構造を変えています）で動くDataset関連のスクリプトを記載します。  \n",
        "自分のデータで動くよう適宜書き換えてください。  \n",
        "Yoloはプログラム部分がヘビーですが、わからないところはそのままコピーして使って頂いて結構です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUhWK3PrXlde"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BTGW1kpaZXs"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhYTzrMFaZUO"
      },
      "outputs": [],
      "source": [
        "def label2yolobox(labels, info_img, maxsize, lrflip):\n",
        "    \"\"\"\n",
        "    Transform coco labels to yolo box labels\n",
        "    Args:\n",
        "        labels (numpy.ndarray): label data whose shape is :math:`(N, 5)`.\n",
        "            Each label consists of [class, x, y, w, h] where \\\n",
        "                class (float): class index.\n",
        "                x, y, w, h (float) : coordinates of \\\n",
        "                    left-top points, width, and height of a bounding box.\n",
        "                    Values range from 0 to width or height of the image.\n",
        "        info_img : tuple of h, w, nh, nw, dx, dy.\n",
        "            h, w (int): original shape of the image\n",
        "            nh, nw (int): shape of the resized image without padding\n",
        "            dx, dy (int): pad size\n",
        "        maxsize (int): target image size after pre-processing\n",
        "        lrflip (bool): horizontal flip flag\n",
        "\n",
        "    Returns:\n",
        "        labels:label data whose size is :math:`(N, 5)`.\n",
        "            Each label consists of [class, xc, yc, w, h] where\n",
        "                class (float): class index.\n",
        "                xc, yc (float) : center of bbox whose values range from 0 to 1.\n",
        "                w, h (float) : size of bbox whose values range from 0 to 1.\n",
        "    \"\"\"\n",
        "    h, w, nh, nw, dx, dy = info_img\n",
        "    x1 = labels[:, 1] / w\n",
        "    y1 = labels[:, 2] / h\n",
        "    x2 = (labels[:, 1] + labels[:, 3]) / w\n",
        "    y2 = (labels[:, 2] + labels[:, 4]) / h\n",
        "    labels[:, 1] = (((x1 + x2) / 2) * nw + dx) / maxsize\n",
        "    labels[:, 2] = (((y1 + y2) / 2) * nh + dy) / maxsize\n",
        "    labels[:, 3] *= nw / w / maxsize\n",
        "    labels[:, 4] *= nh / h / maxsize\n",
        "    if lrflip:\n",
        "        labels[:, 1] = 1 - labels[:, 1]\n",
        "    return labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zA1_los0agWm"
      },
      "outputs": [],
      "source": [
        "def preprocess(img, imgsize, jitter, random_placing=False):\n",
        "    \"\"\"\n",
        "    Image preprocess for yolo input\n",
        "    Pad the shorter side of the image and resize to (imgsize, imgsize)\n",
        "    Args:\n",
        "        img (numpy.ndarray): input image whose shape is :math:`(H, W, C)`.\n",
        "            Values range from 0 to 255.\n",
        "        imgsize (int): target image size after pre-processing\n",
        "        jitter (float): amplitude of jitter for resizing\n",
        "        random_placing (bool): if True, place the image at random position\n",
        "\n",
        "    Returns:\n",
        "        img (numpy.ndarray): input image whose shape is :math:`(C, imgsize, imgsize)`.\n",
        "            Values range from 0 to 1.\n",
        "        info_img : tuple of h, w, nh, nw, dx, dy.\n",
        "            h, w (int): original shape of the image\n",
        "            nh, nw (int): shape of the resized image without padding\n",
        "            dx, dy (int): pad size\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    h, w, _ = img.shape\n",
        "    img = img[:, :, ::-1]\n",
        "    assert img is not None\n",
        "\n",
        "    if jitter > 0:\n",
        "        # add jitter\n",
        "        dw = jitter * w\n",
        "        dh = jitter * h\n",
        "        rand_w = np.random.uniform(low=-dw, high=dw)\n",
        "        rand_h = np.random.uniform(low=-dh, high=dh)\n",
        "        new_ar = (w + rand_w) / (h + rand_h)\n",
        "    else:\n",
        "        new_ar = w / h\n",
        "\n",
        "    if new_ar < 1:\n",
        "        nh = imgsize\n",
        "        nw = nh * new_ar\n",
        "    else:\n",
        "        nw = imgsize\n",
        "        nh = nw / new_ar\n",
        "    nw, nh = int(nw), int(nh)\n",
        "\n",
        "    if random_placing:\n",
        "        dx = int(np.random.uniform(imgsize - nw))\n",
        "        dy = int(np.random.uniform(imgsize - nh))\n",
        "    else:\n",
        "        dx = (imgsize - nw) // 2\n",
        "        dy = (imgsize - nh) // 2\n",
        "\n",
        "    img = cv2.resize(img, (nw, nh))\n",
        "    sized = np.ones((imgsize, imgsize, 3), dtype=np.uint8) * 127\n",
        "    sized[dy:dy+nh, dx:dx+nw, :] = img\n",
        "\n",
        "    # print(\"nh: \", nh, \", nw: \", nw)\n",
        "\n",
        "    info_img = (h, w, nh, nw, dx, dy)\n",
        "    return sized, info_img\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "基本的には以下のコード（MyDataset）を書き換えてもらえれれば、その他のコードはそのままで動きます。\n",
        "\n",
        "ところで、from pycocotools.coco import COCO 等、pycocotools.cocoは比較的簡単にcocoデータを処理してくれる便利ツールですが、ツールを使ってしまうと中で自分が何をしているのかわからなくなってしまうので、使用を控えましょう。"
      ],
      "metadata": {
        "id": "E6J0Lx4syTaV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvjwEtMQaZR-"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import json\n",
        "import cv2\n",
        "import codecs\n",
        "import numpy as np\n",
        "\n",
        "# dataset\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    \"\"\"\n",
        "    dataset class.\n",
        "    img_size=416\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 csv_path=None,\n",
        "                 class_names=None,\n",
        "                 img_size=608,\n",
        "                 augmentation=None,\n",
        "                 min_size=1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "        \"\"\"\n",
        "\n",
        "        if csv_path is None:\n",
        "            raise Exception(\"csv path is not specified.\")\n",
        "\n",
        "        if class_names is None:\n",
        "            raise Exception(\"please specify class names.\")\n",
        "\n",
        "        self.max_labels = 30\n",
        "\n",
        "        self.img_size = img_size\n",
        "        self.min_size = min_size\n",
        "\n",
        "        self.lrflip = augmentation['LRFLIP']\n",
        "        self.jitter = augmentation['JITTER']\n",
        "        self.random_placing = augmentation['RANDOM_PLACING']\n",
        "        self.hue = augmentation['HUE']\n",
        "        self.saturation = augmentation['SATURATION']\n",
        "        self.exposure = augmentation['EXPOSURE']\n",
        "        self.random_distort = augmentation['RANDOM_DISTORT']\n",
        "        self.random_croppig_target = False\n",
        "        self.random_croppig_base = False\n",
        "\n",
        "        _f = open(csv_path, \"r\")\n",
        "        _raw = _f.read()\n",
        "        _raw = _raw.strip()\n",
        "        _csv_list = _raw.split(\"\\n\")\n",
        "\n",
        "        self.csv_list = _csv_list\n",
        "\n",
        "        self.class_names = class_names\n",
        "        self.class_dict = {class_name: i for i, class_name in enumerate(self.class_names)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.csv_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        _row = self.csv_list[index]\n",
        "\n",
        "        img_path = _row.split(\",\")[0]\n",
        "        anno_path = _row.split(\",\")[1]\n",
        "\n",
        "        lrflip = False\n",
        "\n",
        "        # load image and preprocess\n",
        "        img = self._read_image(img_path)\n",
        "\n",
        "        # load labels\n",
        "        boxes, labels = self._get_annotation(anno_path)\n",
        "\n",
        "        img, info_img = preprocess(img,\n",
        "                                   self.img_size,\n",
        "                                   jitter=self.jitter,\n",
        "                                   random_placing=self.random_placing)\n",
        "\n",
        "        img = np.transpose(img / 255., (2, 0, 1))\n",
        "\n",
        "        # concat class + box\n",
        "        label_list = []\n",
        "\n",
        "        for l, b in zip(labels, boxes):\n",
        "\n",
        "            # refine box\n",
        "            x1 = float(b[0])\n",
        "            y1 = float(b[1])\n",
        "            x2 = float(b[2])\n",
        "            y2 = float(b[3])\n",
        "\n",
        "            # seems _get_annotation convert (x1, y1, w, h) > (x1, y1, x2, y2)\n",
        "            b = [x1, y1, x2-x1, y2-y1]\n",
        "\n",
        "            if b[2] > self.min_size and b[3] > self.min_size:\n",
        "                label_list.append([])\n",
        "                label_list[-1].append(l)\n",
        "                label_list[-1].extend(b)\n",
        "\n",
        "        labels = label_list\n",
        "\n",
        "        padded_labels = np.zeros((self.max_labels, 5))\n",
        "        if len(labels) > 0:\n",
        "            labels = np.stack(labels)\n",
        "            labels = label2yolobox(labels, info_img, self.img_size, lrflip)\n",
        "            padded_labels[range(len(labels))[:self.max_labels]] = labels[:self.max_labels]\n",
        "\n",
        "        padded_labels = torch.from_numpy(padded_labels)\n",
        "\n",
        "        return img, padded_labels, info_img\n",
        "\n",
        "    def _get_annotation(self, annotation_file_path):\n",
        "\n",
        "        objects = json.load(codecs.open(annotation_file_path, 'r', 'utf-8-sig'))\n",
        "        objects = objects[\"annotations\"]\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "\n",
        "        for item in objects:\n",
        "\n",
        "            class_name = \"racoon\"\n",
        "\n",
        "            # we're only concerned with clases in our list\n",
        "            if class_name in self.class_dict:\n",
        "\n",
        "                bbox = item[\"bbox\"]\n",
        "                if bbox is not None:\n",
        "\n",
        "                    x1 = bbox.split(\" \")[0]\n",
        "                    y1 = bbox.split(\" \")[1]\n",
        "                    w = bbox.split(\" \")[2]\n",
        "                    h = bbox.split(\" \")[3]\n",
        "\n",
        "                    # VOC dataset format follows Matlab, in which indexes start from 0\n",
        "                    # We do not need this here...\n",
        "                    x1 = float(x1) - 1\n",
        "                    y1 = float(y1) - 1\n",
        "                    x2 = x1 + float(w)\n",
        "                    y2 = y1 + float(h)\n",
        "                    boxes.append([x1, y1, x2, y2])\n",
        "\n",
        "                    # you shold append index of category here\n",
        "                    labels.append(self.class_dict[class_name])\n",
        "\n",
        "        return (np.array(boxes, dtype=np.float32),\n",
        "                np.array(labels, dtype=np.int64))\n",
        "\n",
        "    def _read_image(self, image_path):\n",
        "        # print(\"image_path: \", image_path)\n",
        "        image = cv2.imread(str(image_path))\n",
        "        # print(\"image: \", image.shape)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        return image\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIyT63xIanMt"
      },
      "outputs": [],
      "source": [
        "# ここでDatasetをインスタンス化します。\n",
        "# ここら辺も適宜自分のデータセットに合わせて書き換えてください。\n",
        "\n",
        "csv_path = \"./racoon/train_val.csv\"\n",
        "\n",
        "# 自分のデータセットに合わせて書き換えてください。\n",
        "class_names = [\n",
        "    \"racoon\",\n",
        "]\n",
        "\n",
        "cfg_augmentation = {\n",
        "    \"RANDRESIZE\": False,\n",
        "    \"JITTER\": False,\n",
        "    \"RANDOM_PLACING\": False,\n",
        "    \"HUE\": False,\n",
        "    \"SATURATION\": 1.5,\n",
        "    \"EXPOSURE\": 1.5,\n",
        "    \"LRFLIP\": False,\n",
        "    \"RANDOM_DISTORT\": False,\n",
        "}\n",
        "\n",
        "dataset = MyDataset(csv_path=csv_path, class_names=class_names, augmentation=cfg_augmentation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNGgueoManKd"
      },
      "outputs": [],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiEePOzVdZiI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SujimuaKcxQj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pt9AOLlccyAx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiZraEkDaqxA"
      },
      "outputs": [],
      "source": [
        "# dataloader\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "dataloader  = DataLoader(dataset, batch_size, num_workers=4, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-2VYL69arU_"
      },
      "outputs": [],
      "source": [
        "# データが少なくて評価用と分ける余裕がないので、同じもので擬似的に定義しておきます。\n",
        "\n",
        "val_dataset = MyDataset(csv_path=csv_path, class_names=class_names, augmentation=cfg_augmentation)\n",
        "val_dataloader  = DataLoader(dataset, batch_size, num_workers=4, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiMCDmqDa9fu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrbZxJktcpeb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msq1Dfh2cpZv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ykxDh7AcpXR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fu7UyFGa_3I"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBh-bfVkbAM8"
      },
      "source": [
        "# アーキテクチャ\n",
        "\n",
        "Yoloはアーキテクチャの中にPythonのコードがたくさんあってこれをきちんと理解するのは大変です。  \n",
        "何をやっているのかわからない方はコンセプトだけ理解してそのまま使ってください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdNtBc3Ya_GD"
      },
      "outputs": [],
      "source": [
        "def bboxes_iou(bboxes_a, bboxes_b, xyxy=True):\n",
        "    \"\"\"Calculate the Intersection of Unions (IoUs) between bounding boxes.\n",
        "    IoU is calculated as a ratio of area of the intersection\n",
        "    and area of the union.\n",
        "\n",
        "    Args:\n",
        "        bbox_a (array): An array whose shape is :math:`(N, 4)`.\n",
        "            :math:`N` is the number of bounding boxes.\n",
        "            The dtype should be :obj:`numpy.float32`.\n",
        "        bbox_b (array): An array similar to :obj:`bbox_a`,\n",
        "            whose shape is :math:`(K, 4)`.\n",
        "            The dtype should be :obj:`numpy.float32`.\n",
        "    Returns:\n",
        "        array:\n",
        "        An array whose shape is :math:`(N, K)`. \\\n",
        "        An element at index :math:`(n, k)` contains IoUs between \\\n",
        "        :math:`n` th bounding box in :obj:`bbox_a` and :math:`k` th bounding \\\n",
        "        box in :obj:`bbox_b`.\n",
        "\n",
        "    from: https://github.com/chainer/chainercv\n",
        "    \"\"\"\n",
        "    if bboxes_a.shape[1] != 4 or bboxes_b.shape[1] != 4:\n",
        "        raise IndexError\n",
        "\n",
        "    # top left\n",
        "    if xyxy:\n",
        "        tl = torch.max(bboxes_a[:, None, :2], bboxes_b[:, :2])\n",
        "        # bottom right\n",
        "        br = torch.min(bboxes_a[:, None, 2:], bboxes_b[:, 2:])\n",
        "        area_a = torch.prod(bboxes_a[:, 2:] - bboxes_a[:, :2], 1)\n",
        "        area_b = torch.prod(bboxes_b[:, 2:] - bboxes_b[:, :2], 1)\n",
        "    else:\n",
        "        tl = torch.max((bboxes_a[:, None, :2] - bboxes_a[:, None, 2:] / 2),\n",
        "                        (bboxes_b[:, :2] - bboxes_b[:, 2:] / 2))\n",
        "        # bottom right\n",
        "        br = torch.min((bboxes_a[:, None, :2] + bboxes_a[:, None, 2:] / 2),\n",
        "                        (bboxes_b[:, :2] + bboxes_b[:, 2:] / 2))\n",
        "\n",
        "        area_a = torch.prod(bboxes_a[:, 2:], 1)\n",
        "        area_b = torch.prod(bboxes_b[:, 2:], 1)\n",
        "    en = (tl < br).type(tl.type()).prod(dim=2)\n",
        "    area_i = torch.prod(br - tl, 2) * en  # * ((tl < br).all())\n",
        "    return area_i / (area_a[:, None] + area_b - area_i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ek9kM6HbDhi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "# from utils.utils import bboxes_iou\n",
        "\n",
        "# yolo layer\n",
        "class YOLOLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    detection layer corresponding to yolo_layer.c of darknet\n",
        "    \"\"\"\n",
        "    def __init__(self, config_model, layer_no, in_ch, ignore_thre=0.7):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            config_model (dict) : model configuration.\n",
        "                ANCHORS (list of tuples) :\n",
        "                ANCH_MASK:  (list of int list): index indicating the anchors to be\n",
        "                    used in YOLO layers. One of the mask group is picked from the list.\n",
        "                N_CLASSES (int): number of classes\n",
        "            layer_no (int): YOLO layer number - one from (0, 1, 2).\n",
        "            in_ch (int): number of input channels.\n",
        "            ignore_thre (float): threshold of IoU above which objectness training is ignored.\n",
        "        \"\"\"\n",
        "\n",
        "        super(YOLOLayer, self).__init__()\n",
        "        strides = [32, 16, 8] # fixed\n",
        "        self.anchors = config_model['ANCHORS']\n",
        "        self.anch_mask = config_model['ANCH_MASK'][layer_no]\n",
        "        self.n_anchors = len(self.anch_mask)\n",
        "        self.n_classes = config_model['N_CLASSES']\n",
        "        self.ignore_thre = ignore_thre\n",
        "        self.l2_loss = nn.MSELoss(size_average=False)\n",
        "        # self.bce_loss = nn.BCEWithLogitsLoss(size_average=False)\n",
        "        self.bce_loss = nn.BCELoss(size_average=False)\n",
        "        self.stride = strides[layer_no]\n",
        "        self.all_anchors_grid = [(w / self.stride, h / self.stride)\n",
        "                                 for w, h in self.anchors]\n",
        "        self.masked_anchors = [self.all_anchors_grid[i]\n",
        "                               for i in self.anch_mask]\n",
        "        self.ref_anchors = np.zeros((len(self.all_anchors_grid), 4))\n",
        "        self.ref_anchors[:, 2:] = np.array(self.all_anchors_grid)\n",
        "        self.ref_anchors = torch.FloatTensor(self.ref_anchors)\n",
        "        self.conv = nn.Conv2d(in_channels=in_ch,\n",
        "                              out_channels=self.n_anchors * (self.n_classes + 5),\n",
        "                              kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, xin, labels=None):\n",
        "        \"\"\"\n",
        "        In this\n",
        "        Args:\n",
        "            xin (torch.Tensor): input feature map whose size is :math:`(N, C, H, W)`, \\\n",
        "                where N, C, H, W denote batchsize, channel width, height, width respectively.\n",
        "            labels (torch.Tensor): label data whose size is :math:`(N, K, 5)`. \\\n",
        "                N and K denote batchsize and number of labels.\n",
        "                Each label consists of [class, xc, yc, w, h]:\n",
        "                    class (float): class index.\n",
        "                    xc, yc (float) : center of bbox whose values range from 0 to 1.\n",
        "                    w, h (float) : size of bbox whose values range from 0 to 1.\n",
        "        Returns:\n",
        "            loss (torch.Tensor): total loss - the target of backprop.\n",
        "            loss_xy (torch.Tensor): x, y loss - calculated by binary cross entropy (BCE) \\\n",
        "                with boxsize-dependent weights.\n",
        "            loss_wh (torch.Tensor): w, h loss - calculated by l2 without size averaging and \\\n",
        "                with boxsize-dependent weights.\n",
        "            loss_obj (torch.Tensor): objectness loss - calculated by BCE.\n",
        "            loss_cls (torch.Tensor): classification loss - calculated by BCE for each class.\n",
        "            loss_l2 (torch.Tensor): total l2 loss - only for logging.\n",
        "        \"\"\"\n",
        "\n",
        "        output = self.conv(xin)\n",
        "\n",
        "        batchsize = output.shape[0]\n",
        "        fsize = output.shape[2]\n",
        "        n_ch = 5 + self.n_classes\n",
        "        dtype = torch.cuda.FloatTensor if xin.is_cuda else torch.FloatTensor\n",
        "\n",
        "        output = output.view(batchsize, self.n_anchors, n_ch, fsize, fsize)\n",
        "        output = output.permute(0, 1, 3, 4, 2)  # .contiguous()\n",
        "\n",
        "        # logistic activation for xy, obj, cls\n",
        "        output[..., np.r_[:2, 4:n_ch]] = torch.sigmoid(output[..., np.r_[:2, 4:n_ch]])\n",
        "\n",
        "        # calculate pred - xywh obj cls\n",
        "        x_shift = dtype(np.broadcast_to(np.arange(fsize, dtype=np.float32), output.shape[:4]))\n",
        "        y_shift = dtype(np.broadcast_to(np.arange(fsize, dtype=np.float32).reshape(fsize, 1), output.shape[:4]))\n",
        "        masked_anchors = np.array(self.masked_anchors)\n",
        "\n",
        "        w_anchors = dtype(np.broadcast_to(np.reshape(\n",
        "            masked_anchors[:, 0], (1, self.n_anchors, 1, 1)), output.shape[:4]))\n",
        "        h_anchors = dtype(np.broadcast_to(np.reshape(\n",
        "            masked_anchors[:, 1], (1, self.n_anchors, 1, 1)), output.shape[:4]))\n",
        "\n",
        "        pred = output.clone()\n",
        "        pred[..., 0] = pred[..., 0] + x_shift\n",
        "        pred[..., 1] = pred[..., 1] + y_shift\n",
        "        pred[..., 2] = torch.exp(pred[..., 2]) * w_anchors\n",
        "        pred[..., 3] = torch.exp(pred[..., 3]) * h_anchors\n",
        "\n",
        "        if labels is None:  # not training\n",
        "            pred[..., :4] = pred[..., :4] * self.stride\n",
        "            return pred.reshape(batchsize, -1, n_ch).data\n",
        "\n",
        "        pred = pred[..., :4].data\n",
        "\n",
        "        # target assignment\n",
        "\n",
        "        tgt_mask = torch.zeros(batchsize, self.n_anchors,\n",
        "                               fsize, fsize, 4 + self.n_classes).type(dtype)\n",
        "        obj_mask = torch.ones(batchsize, self.n_anchors,\n",
        "                              fsize, fsize).type(dtype)\n",
        "        tgt_scale = torch.zeros(batchsize, self.n_anchors,\n",
        "                                fsize, fsize, 2).type(dtype)\n",
        "\n",
        "        target = torch.zeros(batchsize, self.n_anchors,\n",
        "                             fsize, fsize, n_ch).type(dtype)\n",
        "\n",
        "        labels = labels.cpu().data\n",
        "        nlabel = (labels.sum(dim=2) > 0).sum(dim=1)  # number of objects\n",
        "\n",
        "        truth_x_all = labels[:, :, 1] * fsize\n",
        "        truth_y_all = labels[:, :, 2] * fsize\n",
        "        truth_w_all = labels[:, :, 3] * fsize\n",
        "        truth_h_all = labels[:, :, 4] * fsize\n",
        "        truth_i_all = truth_x_all.to(torch.int16).numpy()\n",
        "        truth_j_all = truth_y_all.to(torch.int16).numpy()\n",
        "\n",
        "        for b in range(batchsize):\n",
        "            n = int(nlabel[b])\n",
        "            if n == 0:\n",
        "                continue\n",
        "            truth_box = dtype(np.zeros((n, 4)))\n",
        "            truth_box[:n, 2] = truth_w_all[b, :n]\n",
        "            truth_box[:n, 3] = truth_h_all[b, :n]\n",
        "            truth_i = truth_i_all[b, :n]\n",
        "            truth_j = truth_j_all[b, :n]\n",
        "\n",
        "            # calculate iou between truth and reference anchors\n",
        "            anchor_ious_all = bboxes_iou(truth_box.cpu(), self.ref_anchors)\n",
        "            best_n_all = np.argmax(anchor_ious_all, axis=1)\n",
        "            best_n = best_n_all % 3\n",
        "            best_n_mask = ((best_n_all == self.anch_mask[0]) | (\n",
        "                best_n_all == self.anch_mask[1]) | (best_n_all == self.anch_mask[2]))\n",
        "\n",
        "            truth_box[:n, 0] = truth_x_all[b, :n]\n",
        "            truth_box[:n, 1] = truth_y_all[b, :n]\n",
        "\n",
        "            pred_ious = bboxes_iou(\n",
        "                pred[b].reshape(-1, 4), truth_box, xyxy=False)\n",
        "            pred_best_iou, _ = pred_ious.max(dim=1)\n",
        "            pred_best_iou = (pred_best_iou > self.ignore_thre)\n",
        "            pred_best_iou = pred_best_iou.view(pred[b].shape[:3])\n",
        "            # set mask to zero (ignore) if pred matches truth\n",
        "            obj_mask[b] = ~pred_best_iou\n",
        "            # obj_mask[b] = 1 - pred_best_iou\n",
        "\n",
        "            if sum(best_n_mask) == 0:\n",
        "                continue\n",
        "\n",
        "            for ti in range(best_n.shape[0]):\n",
        "                if best_n_mask[ti] == 1:\n",
        "                    i, j = truth_i[ti], truth_j[ti]\n",
        "                    a = best_n[ti]\n",
        "                    obj_mask[b, a, j, i] = 1\n",
        "                    tgt_mask[b, a, j, i, :] = 1\n",
        "                    target[b, a, j, i, 0] = truth_x_all[b, ti] - truth_x_all[b, ti].to(torch.int16).to(torch.float)\n",
        "                    target[b, a, j, i, 1] = truth_y_all[b, ti] - truth_y_all[b, ti].to(torch.int16).to(torch.float)\n",
        "                    target[b, a, j, i, 2] = torch.log(truth_w_all[b, ti] / torch.Tensor(self.masked_anchors)[best_n[ti], 0] + 1e-16)\n",
        "                    target[b, a, j, i, 3] = torch.log(truth_h_all[b, ti] / torch.Tensor(self.masked_anchors)[best_n[ti], 1] + 1e-16)\n",
        "                    target[b, a, j, i, 4] = 1\n",
        "                    target[b, a, j, i, 5 + labels[b, ti, 0].to(torch.int16).numpy()] = 1\n",
        "                    tgt_scale[b, a, j, i, :] = torch.sqrt(2 - truth_w_all[b, ti] * truth_h_all[b, ti] / fsize / fsize)\n",
        "\n",
        "        # loss calculation\n",
        "        output[..., 4] = output[..., 4] * obj_mask\n",
        "        output[..., np.r_[0:4, 5:n_ch]] = output[..., np.r_[0:4, 5:n_ch]] * tgt_mask\n",
        "        output[..., 2:4] = output[..., 2:4] * tgt_scale\n",
        "\n",
        "        target[..., 4] = target[..., 4] * obj_mask\n",
        "        target[..., np.r_[0:4, 5:n_ch]] = target[..., np.r_[0:4, 5:n_ch]] * tgt_mask\n",
        "        target[..., 2:4] = target[..., 2:4] * tgt_scale\n",
        "\n",
        "        bceloss = nn.BCELoss(weight=tgt_scale*tgt_scale,\n",
        "                             size_average=False)  # weighted BCEloss\n",
        "\n",
        "        # bceloss = nn.BCEWithLogitsLoss(weight=tgt_scale*tgt_scale,\n",
        "        #                      size_average=False)  # weighted BCEloss\n",
        "\n",
        "        try:\n",
        "            loss_xy = bceloss(output[..., :2], target[..., :2])\n",
        "        except:\n",
        "            print(\">>>>xy\")\n",
        "            print(\" - target\")\n",
        "            print(target[..., :2])\n",
        "            print(\" - output\")\n",
        "            print(output[..., :2])\n",
        "            print(output.shape)\n",
        "            print(output[..., :2])\n",
        "\n",
        "        loss_wh = self.l2_loss(output[..., 2:4], target[..., 2:4]) / 2\n",
        "\n",
        "        try:\n",
        "            loss_obj = self.bce_loss(output[..., 4], target[..., 4])\n",
        "        except:\n",
        "            print(\">>>>obj\")\n",
        "        try:\n",
        "            loss_cls = self.bce_loss(output[..., 5:], target[..., 5:])\n",
        "        except:\n",
        "            print(\">>>>cls\")\n",
        "\n",
        "        loss_l2 = self.l2_loss(output, target)\n",
        "\n",
        "        loss = loss_xy + loss_wh + loss_obj + loss_cls\n",
        "\n",
        "        return loss, loss_xy, loss_wh, loss_obj, loss_cls, loss_l2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POUbEKhqbFXJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from collections import defaultdict\n",
        "# from models.yolo_layer import YOLOLayer\n",
        "\n",
        "def add_conv(in_ch, out_ch, ksize, stride):\n",
        "    \"\"\"\n",
        "    Add a conv2d / batchnorm / leaky ReLU block.\n",
        "    Args:\n",
        "        in_ch (int): number of input channels of the convolution layer.\n",
        "        out_ch (int): number of output channels of the convolution layer.\n",
        "        ksize (int): kernel size of the convolution layer.\n",
        "        stride (int): stride of the convolution layer.\n",
        "    Returns:\n",
        "        stage (Sequential) : Sequential layers composing a convolution block.\n",
        "    \"\"\"\n",
        "    stage = nn.Sequential()\n",
        "    pad = (ksize - 1) // 2\n",
        "    stage.add_module('conv', nn.Conv2d(in_channels=in_ch,\n",
        "                                       out_channels=out_ch, kernel_size=ksize, stride=stride,\n",
        "                                       padding=pad, bias=False))\n",
        "    stage.add_module('batch_norm', nn.BatchNorm2d(out_ch))\n",
        "    stage.add_module('leaky', nn.LeakyReLU(0.1))\n",
        "    return stage\n",
        "\n",
        "\n",
        "class resblock(nn.Module):\n",
        "    \"\"\"\n",
        "    Sequential residual blocks each of which consists of \\\n",
        "    two convolution layers.\n",
        "    Args:\n",
        "        ch (int): number of input and output channels.\n",
        "        nblocks (int): number of residual blocks.\n",
        "        shortcut (bool): if True, residual tensor addition is enabled.\n",
        "    \"\"\"\n",
        "    def __init__(self, ch, nblocks=1, shortcut=True):\n",
        "\n",
        "        super().__init__()\n",
        "        self.shortcut = shortcut\n",
        "        self.module_list = nn.ModuleList()\n",
        "        for i in range(nblocks):\n",
        "            resblock_one = nn.ModuleList()\n",
        "            resblock_one.append(add_conv(ch, ch//2, 1, 1))\n",
        "            resblock_one.append(add_conv(ch//2, ch, 3, 1))\n",
        "            self.module_list.append(resblock_one)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for module in self.module_list:\n",
        "            h = x\n",
        "            for res in module:\n",
        "                h = res(h)\n",
        "            x = x + h if self.shortcut else h\n",
        "        return x\n",
        "\n",
        "\n",
        "def create_yolov3_modules(config_model, ignore_thre):\n",
        "    \"\"\"\n",
        "    Build yolov3 layer modules.\n",
        "    Args:\n",
        "        config_model (dict): model configuration.\n",
        "            See YOLOLayer class for details.\n",
        "        ignore_thre (float): used in YOLOLayer.\n",
        "    Returns:\n",
        "        mlist (ModuleList): YOLOv3 module list.\n",
        "    \"\"\"\n",
        "\n",
        "    # DarkNet53\n",
        "    mlist = nn.ModuleList()\n",
        "    mlist.append(add_conv(in_ch=3, out_ch=32, ksize=3, stride=1))  # 0\n",
        "    mlist.append(add_conv(in_ch=32, out_ch=64, ksize=3, stride=2)) # 1\n",
        "    mlist.append(resblock(ch=64)) # 2\n",
        "    mlist.append(add_conv(in_ch=64, out_ch=128, ksize=3, stride=2)) # 3\n",
        "    mlist.append(resblock(ch=128, nblocks=2)) # 4\n",
        "    mlist.append(add_conv(in_ch=128, out_ch=256, ksize=3, stride=2)) # 5\n",
        "    mlist.append(resblock(ch=256, nblocks=8))    # shortcut 1 from here # 6\n",
        "    mlist.append(add_conv(in_ch=256, out_ch=512, ksize=3, stride=2)) # 7\n",
        "    mlist.append(resblock(ch=512, nblocks=8))    # shortcut 2 from here # 8\n",
        "    mlist.append(add_conv(in_ch=512, out_ch=1024, ksize=3, stride=2)) # 9\n",
        "    mlist.append(resblock(ch=1024, nblocks=4))  # 10\n",
        "\n",
        "    # YOLOv3\n",
        "    mlist.append(resblock(ch=1024, nblocks=2, shortcut=False))  #11\n",
        "    mlist.append(add_conv(in_ch=1024, out_ch=512, ksize=1, stride=1)) # 12\n",
        "\n",
        "    # 1st yolo branch\n",
        "    mlist.append(add_conv(in_ch=512, out_ch=1024, ksize=3, stride=1)) # 13\n",
        "    mlist.append(YOLOLayer(config_model, layer_no=0, in_ch=1024, ignore_thre=ignore_thre)) # 14\n",
        "\n",
        "    mlist.append(add_conv(in_ch=512, out_ch=256, ksize=1, stride=1)) # 15\n",
        "    mlist.append(nn.Upsample(scale_factor=2, mode='nearest')) # 16\n",
        "    mlist.append(add_conv(in_ch=768, out_ch=256, ksize=1, stride=1)) # 17\n",
        "    mlist.append(add_conv(in_ch=256, out_ch=512, ksize=3, stride=1)) # 18\n",
        "    mlist.append(resblock(ch=512, nblocks=1, shortcut=False)) # 19\n",
        "    mlist.append(add_conv(in_ch=512, out_ch=256, ksize=1, stride=1)) # 20\n",
        "\n",
        "    # 2nd yolo branch\n",
        "    mlist.append(add_conv(in_ch=256, out_ch=512, ksize=3, stride=1)) # 21\n",
        "    mlist.append(YOLOLayer(config_model, layer_no=1, in_ch=512, ignore_thre=ignore_thre)) # 22\n",
        "\n",
        "    mlist.append(add_conv(in_ch=256, out_ch=128, ksize=1, stride=1)) # 23\n",
        "    mlist.append(nn.Upsample(scale_factor=2, mode='nearest')) # 24\n",
        "    mlist.append(add_conv(in_ch=384, out_ch=128, ksize=1, stride=1)) # 25\n",
        "    mlist.append(add_conv(in_ch=128, out_ch=256, ksize=3, stride=1)) # 26\n",
        "    mlist.append(resblock(ch=256, nblocks=2, shortcut=False)) # 27\n",
        "\n",
        "    # yolo branch\n",
        "    mlist.append(YOLOLayer(config_model, layer_no=2, in_ch=256, ignore_thre=ignore_thre)) # 28\n",
        "\n",
        "    return mlist\n",
        "\n",
        "class YOLOv3(nn.Module):\n",
        "    \"\"\"\n",
        "    YOLOv3 model module. The module list is defined by create_yolov3_modules function. \\\n",
        "    The network returns loss values from three YOLO layers during training \\\n",
        "    and detection results during test.\n",
        "    \"\"\"\n",
        "    def __init__(self, config_model, ignore_thre=0.7):\n",
        "        \"\"\"\n",
        "        Initialization of YOLOv3 class.\n",
        "        Args:\n",
        "            config_model (dict): used in YOLOLayer.\n",
        "            ignore_thre (float): used in YOLOLayer.\n",
        "        \"\"\"\n",
        "        super(YOLOv3, self).__init__()\n",
        "\n",
        "        if config_model['TYPE'] == 'YOLOv3':\n",
        "            self.module_list = create_yolov3_modules(config_model, ignore_thre)\n",
        "        else:\n",
        "            raise Exception('Model name {} is not available'.format(config_model['TYPE']))\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        \"\"\"\n",
        "        Forward path of YOLOv3.\n",
        "        Args:\n",
        "            x (torch.Tensor) : input data whose shape is :math:`(N, C, H, W)`, \\\n",
        "                where N, C are batchsize and num. of channels.\n",
        "            targets (torch.Tensor) : label array whose shape is :math:`(N, 50, 5)`\n",
        "\n",
        "        Returns:\n",
        "            training:\n",
        "                output (torch.Tensor): loss tensor for backpropagation.\n",
        "            test:\n",
        "                output (torch.Tensor): concatenated detection results.\n",
        "        \"\"\"\n",
        "        train = targets is not None\n",
        "        output = []\n",
        "        self.loss_dict = defaultdict(float)\n",
        "        route_layers = []\n",
        "\n",
        "\n",
        "        for i, module in enumerate(self.module_list):\n",
        "            # yolo layers\n",
        "            if i in [14, 22, 28]:\n",
        "                if train:\n",
        "                    x, *loss_dict = module(x, targets)\n",
        "                    for name, loss in zip(['xy', 'wh', 'conf', 'cls', 'l2'] , loss_dict):\n",
        "                        self.loss_dict[name] += loss\n",
        "                else:\n",
        "                    x = module(x)\n",
        "                output.append(x)\n",
        "            else:\n",
        "                x = module(x)\n",
        "            # route layers\n",
        "            if i in [6, 8, 12, 20]: # 6,8 = high resolution map, 12,20 = 1 befor yolo map\n",
        "                route_layers.append(x)\n",
        "            if i == 14: # 1st yolo layer\n",
        "                x = route_layers[2] # module_list[12]\n",
        "            if i == 22:  # yolo 2nd\n",
        "                x = route_layers[3] # module_list[20]\n",
        "            if i == 16: # 1st up sampling\n",
        "                x = torch.cat((x, route_layers[1]), 1) # module_list[6]\n",
        "            if i == 24: # 2nd up sampling\n",
        "                x = torch.cat((x, route_layers[0]), 1) # module_list[8]\n",
        "\n",
        "        if train:\n",
        "            return sum(output)\n",
        "        else:\n",
        "            return torch.cat(output, 1)\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここでモデルを定義しています。\n",
        "\n",
        "基本的にそのままでokですが、\"N_CLASSES\": 1の部分はご自身のデータのカテゴリー数に合わせる必要があります。現在1になっているのはアライグマの例ではカテゴリーがアライグマだけなので1個だからです。"
      ],
      "metadata": {
        "id": "Gf81rKeNywu8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vO8RsgzVbKAV"
      },
      "outputs": [],
      "source": [
        "cfg_model = {\n",
        "    \"TYPE\": \"YOLOv3\",\n",
        "    \"BACKBONE\": \"darknet53\",\n",
        "    \"ANCHORS\": [[10, 13], [16, 30], [33, 23],\n",
        "            [30, 61], [62, 45], [59, 119],\n",
        "            [116, 90], [156, 198], [373, 326]],\n",
        "    \"ANCH_MASK\": [[6, 7, 8], [3, 4, 5], [0, 1, 2]],\n",
        "    \"N_CLASSES\": 1,\n",
        "}\n",
        "\n",
        "model = YOLOv3(cfg_model, ignore_thre=0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VJjR2QXbNmZ"
      },
      "outputs": [],
      "source": [
        "# Device\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "model.to(device)\n",
        "_ = model.train()\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KS7bHSjHaB92"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUAmNeCKYxnP"
      },
      "source": [
        "## 3. 学習のループ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8Q2RdM6bzvi"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "subdivision = 16\n",
        "decay = 0.0005\n",
        "momentum = 0.9\n",
        "base_lr = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5svyYsy2bzsJ"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.SGD(\n",
        "        model.parameters(),\n",
        "        lr=base_lr,\n",
        "        momentum=momentum,\n",
        "        dampening=0,\n",
        "        weight_decay=0.0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMxwv7hPbzpX"
      },
      "outputs": [],
      "source": [
        "# scheduler\n",
        "# schedulerはなくても動作すると思いますが、元々のgithubのレポジトリにあったものを念の為コピーしておきます。\n",
        "\n",
        "burn_in = 1000\n",
        "steps = (400000, 450000)\n",
        "\n",
        "\n",
        "# Learning rate setup\n",
        "def burnin_schedule(i):\n",
        "    if i < burn_in:\n",
        "        factor = pow(i / burn_in, 4)\n",
        "    elif i < steps[0]:\n",
        "        factor = 1.0\n",
        "    elif i < steps[1]:\n",
        "        factor = 0.1\n",
        "    else:\n",
        "        factor = 0.01\n",
        "    return factor\n",
        "\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer, burnin_schedule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKigsiG0bzm2"
      },
      "outputs": [],
      "source": [
        "# set validatoin_loss\n",
        "valid_loss_best = 1000000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrCcd0B6cdFO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLDiuNrrcDHc"
      },
      "outputs": [],
      "source": [
        "# 学習のループ\n",
        "\n",
        "# ヒント1:\n",
        "# https://github.com/kouohhashi/PyTorch_YOLOv3/blob/master/train.py を参考にしてください。\n",
        "#\n",
        "# ヒント2:\n",
        "# 学習データですら正しく推論できるようになるまでに3〜5時間以上かかります。\n",
        "# GoogleのColabは1時間放置するとセッションがリセットされてしまいます。また、最大でも10時間（12時間）しか使えないので、\n",
        "# 場合によっては途中経過をtorch.save()で保存しておいてそこから再開しても良いと思います。\n",
        "#\n",
        "# ヒント3:\n",
        "# 物体検知(Object Detection)では比較的大きめの画像をインプットデータとしてつかうため、結果としてバッチサイズは4とかその程度\n",
        "# とれません。それでもokですが、おわかりになるかたはhttps://github.com/kouohhashi/PyTorch_YOLOv3/blob/master/train.py の\n",
        "# コードのように16回に一回勾配を適用する（実質4x16のバッチサイズ64と同じになる。Gradient Accumulationと呼ばれるテクニックです）\n",
        "# やり方を試してみてください。\n",
        "#\n",
        "# ヒント4:\n",
        "# coco evaluationは不要です。tensorboard関連も不要です。\n",
        "\n",
        "# 以下に for ループを作成してください。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUMdQLa4cDFw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w3tuGKrYxnQ"
      },
      "source": [
        "## 4. 学習済みウェイトの保存とロード"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qez-2cHbcDCS"
      },
      "outputs": [],
      "source": [
        "# save\n",
        "\n",
        "import os\n",
        "\n",
        "torch.save({'iter': 0,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            },\n",
        "            \"test.pth\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gurP6qV-cC_u"
      },
      "outputs": [],
      "source": [
        "# モデルを再定義\n",
        "\n",
        "cfg_model = {\n",
        "    \"TYPE\": \"YOLOv3\",\n",
        "    \"BACKBONE\": \"darknet53\",\n",
        "    \"ANCHORS\": [[10, 13], [16, 30], [33, 23],\n",
        "            [30, 61], [62, 45], [59, 119],\n",
        "            [116, 90], [156, 198], [373, 326]],\n",
        "    \"ANCH_MASK\": [[6, 7, 8], [3, 4, 5], [0, 1, 2]],\n",
        "    \"N_CLASSES\": 1,\n",
        "}\n",
        "\n",
        "model = YOLOv3(cfg_model, ignore_thre=0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKDr3lJfcC7y"
      },
      "outputs": [],
      "source": [
        "# ウェイトのロード\n",
        "\n",
        "weight_path = \"test.pth\"\n",
        "state = torch.load(weight_path, map_location=torch.device(\"cuda:0\"))\n",
        "model.load_state_dict(state['model_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7jSYwRwYxnQ"
      },
      "outputs": [],
      "source": [
        "# model settings\n",
        "model.to(device)\n",
        "_ = model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tT46Cpf1fcAi"
      },
      "outputs": [],
      "source": [
        "# 下記はアライグマのサンプルの学習済みウェイトです。\n",
        "# うまくいった時、どんな感じになるかの確認に使ってください。\n",
        "\n",
        "# # download well-trained weights\n",
        "# ! wget https://shabelab.com/mp3/darknet_0706.ckpt\n",
        "\n",
        "# weight_path = \"darknet_0706.ckpt\"\n",
        "# state = torch.load(weight_path, map_location=torch.device(\"cuda:0\"))\n",
        "# model.load_state_dict(state['model_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "374odQbXfb3X"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkgS-4TfYxnR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aI2LxeANYxnR"
      },
      "source": [
        "## 5. テスト"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Icb_VpqYxnR"
      },
      "outputs": [],
      "source": [
        "# テスト画像を準備。とりあえず学習したことが学べているかを確認したいので、\n",
        "# 学習データに存在するものを一つ選択\n",
        "\n",
        "# 本来なら学習に使われていないデータで確認をすべきですが、データ数が少なすぎて\n",
        "# 学習に使われていないデータを正しく推論できるモデルを学習させるのは困難なため、\n",
        "# 学習に使ったデータを指定して頂いて結構でっす。\n",
        "# 少なくとも（過）学習するか確認してみましょう。\n",
        "\n",
        "test_image = \"racoon/images/raccoon-175_jpg.rf.O8xxZ7sVL1v12a5kbTYg.jpg\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UasE0X9xflAP"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "imgsize = 608\n",
        "img_path = test_image\n",
        "img_cv = cv2.imread(img_path, 1)\n",
        "\n",
        "img_raw = img_cv.copy()[:, :, ::-1].transpose((2, 0, 1))\n",
        "\n",
        "img, info_img = preprocess(img_cv, imgsize, jitter=0)  # info = (h, w, nh, nw, dx, dy)\n",
        "img = np.transpose(img / 255., (2, 0, 1))\n",
        "img = torch.from_numpy(img).float().unsqueeze(0)\n",
        "img = Variable(img.type(torch.cuda.FloatTensor))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-JZM6Y6fn2r"
      },
      "outputs": [],
      "source": [
        "\n",
        "def nms(bbox, thresh, score=None, limit=None):\n",
        "    \"\"\"Suppress bounding boxes according to their IoUs and confidence scores.\n",
        "    Args:\n",
        "        bbox (array): Bounding boxes to be transformed. The shape is\n",
        "            :math:`(R, 4)`. :math:`R` is the number of bounding boxes.\n",
        "        thresh (float): Threshold of IoUs.\n",
        "        score (array): An array of confidences whose shape is :math:`(R,)`.\n",
        "        limit (int): The upper bound of the number of the output bounding\n",
        "            boxes. If it is not specified, this method selects as many\n",
        "            bounding boxes as possible.\n",
        "    Returns:\n",
        "        array:\n",
        "        An array with indices of bounding boxes that are selected. \\\n",
        "        They are sorted by the scores of bounding boxes in descending \\\n",
        "        order. \\\n",
        "        The shape of this array is :math:`(K,)` and its dtype is\\\n",
        "        :obj:`numpy.int32`. Note that :math:`K \\\\leq R`.\n",
        "\n",
        "    from: https://github.com/chainer/chainercv\n",
        "    \"\"\"\n",
        "\n",
        "    if len(bbox) == 0:\n",
        "        return np.zeros((0,), dtype=np.int32)\n",
        "\n",
        "    if score is not None:\n",
        "        order = score.argsort()[::-1]\n",
        "        bbox = bbox[order]\n",
        "    bbox_area = np.prod(bbox[:, 2:] - bbox[:, :2], axis=1)\n",
        "\n",
        "    selec = np.zeros(bbox.shape[0], dtype=bool)\n",
        "    for i, b in enumerate(bbox):\n",
        "        tl = np.maximum(b[:2], bbox[selec, :2])\n",
        "        br = np.minimum(b[2:], bbox[selec, 2:])\n",
        "        area = np.prod(br - tl, axis=1) * (tl < br).all(axis=1)\n",
        "\n",
        "        iou = area / (bbox_area[i] + bbox_area[selec] - area)\n",
        "        if (iou >= thresh).any():\n",
        "            continue\n",
        "\n",
        "        selec[i] = True\n",
        "        if limit is not None and np.count_nonzero(selec) >= limit:\n",
        "            break\n",
        "\n",
        "    selec = np.where(selec)[0]\n",
        "    if score is not None:\n",
        "        selec = order[selec]\n",
        "    return selec.astype(np.int32)\n",
        "\n",
        "def postprocess(prediction, num_classes, conf_thre=0.7, nms_thre=0.45):\n",
        "    \"\"\"\n",
        "    Postprocess for the output of YOLO model\n",
        "    perform box transformation, specify the class for each detection,\n",
        "    and perform class-wise non-maximum suppression.\n",
        "    Args:\n",
        "        prediction (torch tensor): The shape is :math:`(N, B, 4)`.\n",
        "            :math:`N` is the number of predictions,\n",
        "            :math:`B` the number of boxes. The last axis consists of\n",
        "            :math:`xc, yc, w, h` where `xc` and `yc` represent a center\n",
        "            of a bounding box.\n",
        "        num_classes (int):\n",
        "            number of dataset classes.\n",
        "        conf_thre (float):\n",
        "            confidence threshold ranging from 0 to 1,\n",
        "            which is defined in the config file.\n",
        "        nms_thre (float):\n",
        "            IoU threshold of non-max suppression ranging from 0 to 1.\n",
        "\n",
        "    Returns:\n",
        "        output (list of torch tensor):\n",
        "\n",
        "    \"\"\"\n",
        "    box_corner = prediction.new(prediction.shape)\n",
        "    box_corner[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2\n",
        "    box_corner[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2\n",
        "    box_corner[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2\n",
        "    box_corner[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2\n",
        "    prediction[:, :, :4] = box_corner[:, :, :4]\n",
        "\n",
        "    output = [None for _ in range(len(prediction))]\n",
        "    for i, image_pred in enumerate(prediction):\n",
        "        # Filter out confidence scores below threshold\n",
        "        class_pred = torch.max(image_pred[:, 5:5 + num_classes], 1)\n",
        "        class_pred = class_pred[0]\n",
        "        conf_mask = (image_pred[:, 4] * class_pred >= conf_thre).squeeze()\n",
        "        image_pred = image_pred[conf_mask]\n",
        "\n",
        "        # If none are remaining => process next image\n",
        "        if not image_pred.size(0):\n",
        "            continue\n",
        "        # Get detections with higher confidence scores than the threshold\n",
        "        ind = (image_pred[:, 5:] * image_pred[:, 4][:, None] >= conf_thre).nonzero()\n",
        "        # Detections ordered as (x1, y1, x2, y2, obj_conf, class_conf, class_pred)\n",
        "        detections = torch.cat((\n",
        "                image_pred[ind[:, 0], :5],\n",
        "                image_pred[ind[:, 0], 5 + ind[:, 1]].unsqueeze(1),\n",
        "                ind[:, 1].float().unsqueeze(1)\n",
        "                ), 1)\n",
        "        # Iterate through all predicted classes\n",
        "        unique_labels = detections[:, -1].cpu().unique()\n",
        "        if prediction.is_cuda:\n",
        "            unique_labels = unique_labels.cuda()\n",
        "        for c in unique_labels:\n",
        "            # Get the detections with the particular class\n",
        "            detections_class = detections[detections[:, -1] == c]\n",
        "            nms_in = detections_class.cpu().numpy()\n",
        "            nms_out_index = nms(\n",
        "                nms_in[:, :4], nms_thre, score=nms_in[:, 4]*nms_in[:, 5])\n",
        "            detections_class = detections_class[nms_out_index]\n",
        "            if output[i] is None:\n",
        "                output[i] = detections_class\n",
        "            else:\n",
        "                output[i] = torch.cat((output[i], detections_class))\n",
        "\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpMOMwxNfojm"
      },
      "outputs": [],
      "source": [
        "num_classes = 1\n",
        "confthre = 0.2\n",
        "nmsthre = 0.45\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(img)\n",
        "    outputs = postprocess(outputs, num_classes, confthre, nmsthre)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWmZ9qrpfohl"
      },
      "outputs": [],
      "source": [
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jaYWueVfodB"
      },
      "outputs": [],
      "source": [
        "def yolobox2label(box, info_img):\n",
        "    \"\"\"\n",
        "    Transform yolo box labels to yxyx box labels.\n",
        "    Args:\n",
        "        box (list): box data with the format of [yc, xc, w, h]\n",
        "            in the coordinate system after pre-processing.\n",
        "        info_img : tuple of h, w, nh, nw, dx, dy.\n",
        "            h, w (int): original shape of the image\n",
        "            nh, nw (int): shape of the resized image without padding\n",
        "            dx, dy (int): pad size\n",
        "    Returns:\n",
        "        label (list): box data with the format of [y1, x1, y2, x2]\n",
        "            in the coordinate system of the input image.\n",
        "    \"\"\"\n",
        "    h, w, nh, nw, dx, dy = info_img\n",
        "    y1, x1, y2, x2 = box\n",
        "    h = float(h)\n",
        "    w = float(w)\n",
        "    nh = float(nh)\n",
        "    nw = float(nw)\n",
        "    dx = float(dx)\n",
        "    dy = float(dy)\n",
        "\n",
        "    box_h = ((y2 - y1) / nh) * h\n",
        "    box_w = ((x2 - x1) / nw) * w\n",
        "    y1 = ((y1 - dy) / nh) * h\n",
        "    x1 = ((x1 - dx) / nw) * w\n",
        "    label = [y1, x1, y1 + box_h, x1 + box_w]\n",
        "    return label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUvVazSDfoZd"
      },
      "outputs": [],
      "source": [
        "# カテゴリー名を設定。自分のデータに合わせて変更してください。\n",
        "\n",
        "class_names = [\n",
        "    \"racoon\",\n",
        "]\n",
        "\n",
        "if outputs[0] is None:\n",
        "    print(\"no object detected.\")\n",
        "else:\n",
        "    class_names = class_names\n",
        "    bboxes = list()\n",
        "\n",
        "    for x1, y1, x2, y2, conf, cls_conf, cls_pred in outputs[0]:\n",
        "        # print(x1, y1, x2, y2)\n",
        "        box = yolobox2label([y1, x1, y2, x2], info_img)\n",
        "        bboxes.append(box)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILYYRmwUfwyb"
      },
      "outputs": [],
      "source": [
        "from skimage import io\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "\n",
        "im = Image.open(test_image)\n",
        "\n",
        "# Create figure and axes\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.imshow(im)\n",
        "\n",
        "\n",
        "for bbox in bboxes:\n",
        "\n",
        "    int_bbox = [int(e) for e in bbox]\n",
        "\n",
        "    # rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2]-bbox[0], bbox[3]-bbox[1], linewidth=3, edgecolor='b', facecolor='none')\n",
        "    rect = patches.Rectangle((int_bbox[1], int_bbox[0]), int_bbox[3]-int_bbox[1], int_bbox[2]-int_bbox[0], linewidth=3, edgecolor='b', facecolor='none')\n",
        "\n",
        "    # Display the image\n",
        "    ax.add_patch(rect)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyjKFyorf-t_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WUAmNeCKYxnP",
        "7w3tuGKrYxnQ",
        "aI2LxeANYxnR"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}